{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0e97eb-c470-43d9-a8d4-ddc1f1a1142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02c5159a-a6e2-4994-919b-f968b6ec0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0564408-d29f-4347-befd-89d4f469a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kXborg/vlms.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7881b24b-4a42-465a-898a-12a1c3b2e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/vlms\n"
     ]
    }
   ],
   "source": [
    "%cd vlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add931a6-8502-4486-9273-308d72137a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments  README.md\tResults  dataset\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e144f8b-636c-4c58-a267-1dca74b4cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75962543-3bc0-4884-991c-bee7b6c6f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 14:49:07 [utils.py:253] non-default args: {'enable_prefix_caching': False, 'disable_log_stats': True, 'mm_processor_cache_gb': 0, 'logits_processors': [<class 'vllm.model_executor.models.deepseek_ocr.NGramPerReqLogitsProcessor'>], 'model': 'deepseek-ai/DeepSeek-OCR'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b69dd32b83407fa44cfbab79a27fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 14:49:15 [model.py:631] Resolved architecture: DeepseekOCRForCausalLM\n",
      "INFO 11-13 14:49:15 [model.py:1736] Using max model len 8192\n",
      "INFO 11-13 14:49:19 [scheduler.py:254] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64ebaec42534073b76f703629577666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39c5cd4e2a7469197956d3979c2e777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c19434cdea4bd19daa61dce3bab193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:21 [core.py:94] Initializing a V1 LLM engine (v0.11.1rc7.dev121+g86d15bfd8) with config: model='deepseek-ai/DeepSeek-OCR', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-OCR', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-OCR, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': True, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:23 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01ed310e72f40169e2ad85e9a1eaa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:27 [gpu_model_runner.py:3047] Starting to load model deepseek-ai/DeepSeek-OCR...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:28 [layer.py:569] MultiHeadAttention attn_backend: AttentionBackendEnum.FLASH_ATTN, use_upstream_fa: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspace/vllm/lib/python3.12/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py\", line 836, in <module>\n",
      "    main()\n",
      "  File \"/workspace/vllm/lib/python3.12/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py\", line 829, in main\n",
      "    build_ninja(build_dir=str(build_dir))\n",
      "  File \"/workspace/vllm/lib/python3.12/site-packages/tvm_ffi/cpp/extension.py\", line 344, in build_ninja\n",
      "    status = subprocess.run(check=False, args=command, cwd=build_dir, capture_output=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ninja'\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m /workspace/vllm/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:106: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m You may try AOT-module via `pip install torch-c-dlpack-ext`\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:30 [cuda.py:408] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:30 [cuda.py:417] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:49:30 [layer.py:342] Enabled separate cuda stream for MoE shared_experts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea58d74c97e4246ba55d1b3762a596f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:22 [weight_utils.py:441] Time spent downloading weights for deepseek-ai/DeepSeek-OCR: 51.269518 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd9346b87384f79b124db32ae74e422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0aa70412e6483d93c918ddbeb250b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:24 [default_loader.py:314] Loading weights took 1.84 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:25 [gpu_model_runner.py:3126] Model loading took 6.2319 GiB memory and 56.204229 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:25 [gpu_model_runner.py:3876] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 11 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:31 [backends.py:618] Using cache directory: /root/.cache/vllm/torch_compile_cache/5223ca70e3/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:31 [backends.py:634] Dynamo bytecode transform time: 3.67 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:33 [backends.py:250] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:47 [backends.py:281] Compiling a graph for dynamic shape takes 15.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m /workspace/vllm/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS=\"+dynamo\" for a DEBUG stack trace.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m   torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m WARNING 11-13 14:50:50 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/workspace/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=896,device_name=NVIDIA_RTX_A6000.json']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:51 [monitor.py:34] torch.compile takes 19.31 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:52 [gpu_worker.py:353] Available KV cache memory: 34.92 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:53 [kv_cache_utils.py:1229] GPU KV cache size: 610,272 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:50:53 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 74.50x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.43it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:51:05 [gpu_model_runner.py:4032] Graph capturing finished in 12 secs, took 0.52 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3959)\u001b[0;0m INFO 11-13 14:51:05 [core.py:247] init engine (profile, create kv cache, warmup model) took 40.11 seconds\n",
      "INFO 11-13 14:51:06 [llm.py:353] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "llm = LLM(\n",
    "    model=\"deepseek-ai/DeepSeek-OCR\",\n",
    "    enable_prefix_caching=False,\n",
    "    mm_processor_cache_gb=0,\n",
    "    logits_processors=[NGramPerReqLogitsProcessor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd32e1b3-f155-4b1c-9784-f09427458d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a1866bb07740ff952bbecbb91c85e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e664e3e8de264c0b8c4d331d85fab608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process the images: 3.71s\n",
      "HDW070_008LZ02 数据手册\n",
      "\n",
      "Professional, Creditable, Successful\n",
      "\n",
      "HDW070_008LZ02\n",
      "\n",
      "7 英寸，1024×600 像素点阵，16.7M 色，IPS 屏\n",
      "\n",
      "电容触摸屏，HDMI 多媒体显示屏\n",
      "\n",
      "7 Inch, 1024xRGBx600, 16.7M Colors, IPS screen\n",
      "\n",
      "CTP, HDMI multimedia display\n",
      "\n",
      "修订记录 Record of Revision\n",
      "\n",
      "| 版本/Ver | 日期/Revise Date | 描述/Content | 编辑人/Editor |\n",
      "|---------|-----------------|--------------|--------------|\n",
      "| 00      | 2022-04-14      | 首次发布 First Edition | 郑运佳 |\n",
      "| 01      | 2022-07-08      | 更新实物图 Update the physical drawing | 郑运佳 |\n",
      "\n",
      "迪文科技 DWIN Technologies 400 018 9008 www.dwin.com.cn\n",
      "显示性能参数 Display\n",
      "\n",
      "| 参数 Item | 数据 Parameter | 说明 Description |\n",
      "|-----------|----------------|----------------|\n",
      "| 颜色 Color | 16.7M(16777216)colors | 24 bit color 8R8G8B |\n",
      "| 液晶类型 Panel Type | IPS | IPS 工艺 TFT 显示屏，宽视角 IPS TFT LCM, wide viewing angle |\n",
      "| 可视角度 Viewing Angle | 85/85/85/85 (L/R/U/D) | 最佳视角：上下左右对称 Best View: symmetrical |\n",
      "| 显示尺寸 Active Area (A.A.) | 154.08(W) × 85.92(H) | 1024x600 |\n",
      "| 视域尺寸 View Area (V.A.) | 154.08(W) × 85.92(H) | 1024x600 |\n",
      "| 分辨率 Resolution | 1024x600 | - |\n",
      "| 背光模式 Backlight | LED | 不低于 20000H（以最高亮度连续工作，亮度减半时间） ≥20000H(Continuous working, with maximum brightness, time of the brightness decays to 50%) |\n",
      "| 亮度 Brightness | 600nit | - |\n",
      "| 注：超过 30 分钟长时间显示高对比度静止画面可能导致显示残影，请增加屏保避免该问题。Note: Displaying of high-contrast still images for more than 30 minutes may result in residual images. Please add animation to avoid this problem. |\n",
      "\n",
      "电能性能参数 Voltage & Current\n",
      "\n",
      "| 参数 Item | 说明 Descriptions |\n",
      "|-----------|----------------|\n",
      "| 工作电压 Power Voltage | 6~36V，典型值 12V |\n",
      "| 电流参数 Current parameters | 720mA | VCC=6V（最低启动电压） VCC=6V (Minimum starting voltage) |\n",
      "|  | 330mA | VCC=12V（标准供电电压） VCC=12V (Standard supply voltage) |\n",
      "|  | 140mA | VCC=36V（最高供电电压） VCC=36V (Maximum supply voltage) |\n",
      "| 推荐工作电源：12V 1A 的直流稳压电源 Recommended power supply: 12V 1A DC |\n",
      "\n",
      "工作环境和可靠性参数 Reliability Test\n",
      "\n",
      "| 参数 Item | 测试条件 Conditions | 最小值 Min | 典型值 Typ | 最大值 Max | 单位 Unit |\n",
      "|-----------|----------------------|-------------|------------|------------|-----------|\n",
      "| 工作温度 Working Temperature | 12V 电压下，湿度 60% 60%RH at 12V voltage | -20 | 25 | 70 | ℃ |\n",
      "| 储存温度 Storage Temperature | - | -30 | 25 | 80 | ℃ |\n",
      "| 工作湿度 Working Humidity | 25℃ | 10% | 60% | 90% | RH |\n",
      "| 静电放电测试 Electrostatic discharge test | 接触放电 Contact discharge | - | ±6KV | - | - |\n",
      "|  | 空气放电 Air discharge | - | ±8KV | - | - |\n",
      "| 电快速瞬变脉冲群 EFT | - | - | ±2KV | - | - |\n",
      "\n",
      "接口性能参数 Interface\n",
      "\n",
      "| 参数 Item | 测试条件 Conditions | 最小值 Min | 典型值 Typ | 最大值 Max | 单位 Unit |\n",
      "|-----------|----------------------|-------------|------------|------------|-----------|\n",
      "| 用户接口方式 Socket | 电源接口，HDMI 接口 power interface, HDMI interface | - | - | - | - |\n",
      "| USB 接口 | 有（外接 USB 电容触摸屏） Yes (Connect USB capacitive touch screen) | - | - | - | - |\n",
      "| SD 卡接口 SD Slot | 无 None | - | - | - | - |\n",
      "\n",
      "迪文科技 DWIN Technologies 400 018 9008 www.dwin.com.cn\n",
      "专业素养、诚实守信、追求卓越  \n",
      "Professional, Creditable, Successful  \n",
      "\n",
      "HDW070_008LZ02_数据手册  \n",
      "Product Specification  \n",
      "\n",
      "外设支持 Peripheral  \n",
      "| 支持外设 | 支持电容式触摸屏 |\n",
      "| --- | --- |\n",
      "| Peripheral | Capacitive touch panel |\n",
      "\n",
      "包装和物理尺寸 Packing Capacity & Dimension  \n",
      "| 尺寸 Dimension |  |\n",
      "| --- | --- |\n",
      "| 外形尺寸 Dimension | 184.08(W)×115.92(H)×17.75(T)mm |\n",
      "| 净重量 Net Weight | 200g |\n",
      "\n",
      "声明：产品设计改善或变更，不单独另行通知。  \n",
      "Disclaimer: The product design is subject to alternation and improvement without prior notice.  \n",
      "\n",
      "迪文科技 DWIN Technologies  \n",
      "400 018 9008  \n",
      "www.dwin.com.cn\n"
     ]
    }
   ],
   "source": [
    "# Prepare batched input with your image file\n",
    "image_1 = Image.open(\"dataset/pdf-images2/1.jpg\").convert(\"RGB\")\n",
    "image_2 = Image.open(\"dataset/pdf-images2/2.jpg\").convert(\"RGB\")\n",
    "image_3 = Image.open(\"dataset/pdf-images2/3.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Updated prompt to request Markdown + English\n",
    "prompt = \"<image>\\nFree OCR. Convert to markdown.\"\n",
    "\n",
    "model_input = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_1}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_2}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_3}\n",
    "    },\n",
    "]\n",
    "\n",
    "sampling_param = SamplingParams(\n",
    "            temperature=0.0,\n",
    "            max_tokens=8192,\n",
    "            # ngram logit processor args\n",
    "            extra_args=dict(\n",
    "                ngram_size=30,\n",
    "                window_size=90,\n",
    "                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n",
    "            ),\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "t1 = time.time()\n",
    "# Generate output\n",
    "model_outputs = llm.generate(model_input, sampling_param)\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Time taken to process the images: {round(t2-t1, 2)}s\")\n",
    "\n",
    "# Print output\n",
    "for output in model_outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd146d-9ea3-4608-8517-5d0d4c590b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
