| Feature| **Moondream 1**| **Moondream 2**| **Moondream 3 (Preview)**|
| -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Parameter count / scale**| 1.6B parameters. (Vision encoder + LM backbone)| 1.86B parameters. Slightly larger.| 9B total, 2B active (because of MoE).|
| **Vision encoder**| Based on SigLIP with a ViT-style architecture. Depth 12 transformer blocks; hidden dim 1152; MLP \~4304 → 1152 etc. (no MoE in vision) | Builds on same SigLIP vision encoder; similar hidden size etc, but deeper and more refined.| SigLIP-based vision encoder still, but improved: supports multi-crop channel concatenation for higher resolution images more efficiently.|
| **Language Backbone / LM** | Uses Phi-1.5 LM for text side.| Same / initialized from Phi-1.5. Probably similar transformer LM architecture.| Larger LM hidden dimension: hidden dim = 2048; uses mixture-of-experts (MoE) FFNs in most layers.|
| **Depth / Layers**| 12 vision transformer blocks. (Vision side) Language side 24 transformer blocks?| More vision blocks:  \~27 vision transformer blocks. LM side similar count.| 24 total layers: first 4 dense; remaining 20 use MoE FFNs.|
| **Hidden dimensions / inner-MLP sizes**| Vision hidden dim 1152; MLP inner size \~4304. LM side hidden \~2048.| Similar to 1 for vision; likely a bit optimised.| Vision + LM hidden dim = 2048 in LM part; MoE FFN inner/gate dim = 1024.|
| **Mixture-of-Experts (MoE)**| No MoE. Dense model.| No MoE; dense.| Yes: MoE FFNs — 64 experts, with \~8 experts activated per token.|
| **Context length (text)**| Limited to “typical” lengths — 4K tokens or similar.| Same \~4K tokens context in training/inference.| Up to **32K token context** (extended) with learned temperature scaling etc.|
| **Attention / positional improvements** | Standard transformer attention; positional encodings typical.| Similar attention + standard positional encodings.| Multi-headed attention with **learned position- and data-dependent temperature scaling**. Also some optimisations for long context.|
| **OCR / Document / Visual Reasoning capability**| Basic – image-captioning, VQA, etc. OCR less strong, less document-querying. | Improved: Moondream2 has better vision-language tasks, more image-text alignment etc. But still limitations in complex OCR, etc.  | Strong improvements: better OCR, document understanding, charts, counting, and structured output (like JSON) etc. Vision reasoning is a key goal.|
| **Efficiency / Active inference / Resource usage** | More lightweight than 2 and 3; good for limited hardware. | Designed for edge / resource constrained hardware; small, faster inference than large models.| More compute overall, but MoE and “active” parameters mean lower effective compute per token; still trying to keep deployment efficiency. Also optimizations with token efficient vision processing.  |
| **Trade-offs or complexity** | Simpler model; easier to understand/tune; less capacity.| More capacity, but still dense → growing compute cost with depth.                                                                                        | More complexity: MoE, longer context, more intricate architecture (vision optimizations, temp scaling etc.), more challenging to fully train / deploy.                                                                   |
