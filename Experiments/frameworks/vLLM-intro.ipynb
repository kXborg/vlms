{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf669a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize model once\n",
    "llm = LLM(\n",
    "    model=\"LiquidAI/LFM2-1.2B\",\n",
    "    gpu_memory_utilization=0.7,\n",
    ")\n",
    "\n",
    "# You can tweak these later based on how the model feels\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,      # >0 => sampling; 0 = greedy\n",
    "    top_p=0.95,           # nucleus sampling\n",
    "    max_tokens=256,       # max tokens to generate\n",
    "    repetition_penalty=1.05,  # small penalty to avoid loops\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0819cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(user_prompt: str) -> str:\n",
    "    system_prompt = \"\"\"<|im_start|>system\\nYou are a helpful and concise assistant.\\n<|im_end|>\"\"\"\n",
    "    user_part = f\"<|im_start|>user\\n{user_prompt}\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    return system_prompt + user_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a11b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt: str):\n",
    "    final_prompt = build_prompt(prompt)\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        prompts=[final_prompt],\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "\n",
    "    text = outputs[0].outputs[0].text\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbd5e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8914e257c741fba25479924b141b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf439ab348c46cd9dedf16dddd39a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is an AI and I don't have a personal name but I can operate under various names depending on the context. For example, in conversation, I might call myself \"Assistant\" or simply \"AI\". However, if you want me to be named formally, I can refer to myself as \"Model Assistant\". How would you like me to be called?\n"
     ]
    }
   ],
   "source": [
    "obj = infer(\"Hi, What's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0b6b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178c2bb3cc064d728009613702088a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d708fa5ee342d787ddf1ac35d5b3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, exactly. I'm ready to assist you as your Liquid AI-powered conversational assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "obj = infer(\"I am going to call you liquid AI. Does that sound ok?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbbb286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c9ddef59994338a1c7fcd350e5594e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90d96db42054ad5a5c45199b5f1bde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is an artificial intelligence, so I don't have a personal name. I'm more accurately described as a conversational AI model. However, for the sake of your request, I can say my \"name\" would be \"EchoBot.\"\n"
     ]
    }
   ],
   "source": [
    "obj = infer(\"Now tell me what's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d42d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9165b4d9a7254472b831fa93a541c523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bb9174348d450a900e95e84b3bab69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.AI does not currently have a publicly listed CEO, as the company was reportedly acquired in 2022. If information about the new leadership isn't widely reported or publicized at the time of your query, you might need to check recent news sources for updates on who holds the CEO position now.\n"
     ]
    }
   ],
   "source": [
    "obj = infer(\"Who is the CEO of X.AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1466e971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c0ffd471af4e9fbf0caabad3d3a51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5642ce8d8fbc470383a9bf50fe9560f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current CEO of Adobe is Shantanu Narayen, who has been in the role since February 6, 2007. However, it's always good to check recent sources as titles may change.\n"
     ]
    }
   ],
   "source": [
    "obj = infer('Who is the CEO of adobe?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2f573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c59e9249a943eabbcf9ab3f13b6e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c83c053f3d47a7908341c960982f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004 * 12 + 13 * 2 = 24048 + 26 = 24074\n"
     ]
    }
   ],
   "source": [
    "obj = infer(\"2004x12+13*2 = ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "442cd02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e335f68c920443db9678971cd7005ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94fa93801e242e4929c0f7e1634b282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, I cannot directly process images myself. However, I can work with image descriptions or text related to images. If you describe an image or provide details about what you see in it, I can certainly assist in providing information, answering questions, or generating creative content based on that description.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, I cannot directly process images myself. However, I can work with image descriptions or text related to images. If you describe an image or provide details about what you see in it, I can certainly assist in providing information, answering questions, or generating creative content based on that description.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('Can you process images?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9ec1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5389e5195bae4128897178ac5cfd0e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f7d24bb7454fd78352f09d5053395f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I can accept text files as inputs. However, please note that to interact with text files, you typically need to either provide the file directly (if it's small enough for your environment) or upload it through an external interface like an API if we're in a conversation context. If you upload the file via an interface, it would be processed programmatically based on the interface's specifications. For direct file input, ensure the file is in a format compatible with my processing capabilities, usually plain text files with no specific formatting required.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, I can accept text files as inputs. However, please note that to interact with text files, you typically need to either provide the file directly (if it's small enough for your environment) or upload it through an external interface like an API if we're in a conversation context. If you upload the file via an interface, it would be processed programmatically based on the interface's specifications. For direct file input, ensure the file is in a format compatible with my processing capabilities, usually plain text files with no specific formatting required.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('Can you accept text files as inputs?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3b50af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050e001c96174713a436d5afbde5d4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32cf703e32e4ec1998918a2d46daffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is a Python script that demonstrates how to load and display images in a Jupyter Notebook using OpenCV:\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load images\n",
      "def load_image(image_path):\n",
      "    image = cv2.imread(image_path)\n",
      "    if image is None:\n",
      "        print(f\"Error: Could not load image from {image_path}\")\n",
      "        return None\n",
      "    return image\n",
      "\n",
      "# Display image\n",
      "def display_image(image):\n",
      "    plt.figure(figsize=(10, 5))\n",
      "    plt.subplot(1, 2, 1), plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)), plt.title('Original Image')\n",
      "    plt.subplot(1, 2, 2), plt.imshow(cv2.cvtColor(np.array(image)/255.0, cv2.COLOR_RGB2BGR)), plt.title('Normalized Image')\n",
      "    plt.show()\n",
      "\n",
      "# Main function\n",
      "def main(image_path):\n",
      "    image = load_image(image_path)\n",
      "    if image is not None:\n",
      "       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Certainly! Below is a Python script that demonstrates how to load and display images in a Jupyter Notebook using OpenCV:\\n\\n```python\\nimport cv2\\nimport matplotlib.pyplot as plt\\n\\n# Load images\\ndef load_image(image_path):\\n    image = cv2.imread(image_path)\\n    if image is None:\\n        print(f\"Error: Could not load image from {image_path}\")\\n        return None\\n    return image\\n\\n# Display image\\ndef display_image(image):\\n    plt.figure(figsize=(10, 5))\\n    plt.subplot(1, 2, 1), plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)), plt.title(\\'Original Image\\')\\n    plt.subplot(1, 2, 2), plt.imshow(cv2.cvtColor(np.array(image)/255.0, cv2.COLOR_RGB2BGR)), plt.title(\\'Normalized Image\\')\\n    plt.show()\\n\\n# Main function\\ndef main(image_path):\\n    image = load_image(image_path)\\n    if image is not None:\\n       '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('write python code to load and display images in jupyter notebook using opencv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4211453",
   "metadata": {},
   "source": [
    "## Token by Token Printing \n",
    "Streaming using `LLMEngine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a7f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.engine.arg_utils import EngineArgs\n",
    "from vllm.engine.llm_engine import LLMEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2a02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Engine Args\n",
    "engine_args = EngineArgs(\n",
    "    model=\"LiquidAI/LFM2-1.2B\",\n",
    "    gpu_memory_utilization=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd34890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine\n",
    "engine = LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d4bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "758aa58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def infer_stream(prompt: str):\n",
    "    request_id = \"req0\"\n",
    "    \n",
    "    # Submit the request for generation\n",
    "    engine.add_request(\n",
    "        request_id=request_id,\n",
    "        prompt=build_prompt(prompt),\n",
    "        params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    prev_text = \"\"\n",
    "    final_text = \"\" \n",
    "    \n",
    "    while engine.has_unfinished_requests():\n",
    "        request_outputs = engine.step()\n",
    "        \n",
    "        for ro in request_outputs:\n",
    "            if ro.request_id != request_id:\n",
    "                continue\n",
    "            \n",
    "            full_text = ro.outputs[0].text \n",
    "            new_chunk = full_text[len(prev_text):]\n",
    "            \n",
    "            if new_chunk:\n",
    "                print(new_chunk, end=\"\", flush=True)\n",
    "                prev_text = full_text\n",
    "            \n",
    "            if ro.finished:\n",
    "                print()\n",
    "                final_text = full_text\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                token_count = len(final_text.split())\n",
    "                \n",
    "                print(\"\\n===== Inference Stats =====\")\n",
    "                print(f\"Time taken:       {duration:.2f} sec\")\n",
    "                print(f\"Token count:      {token_count}\")\n",
    "                print(f\"Throughput:       {token_count / duration:.2f} tokens/sec\")\n",
    "                print(\"============================\\n\")\n",
    "                \n",
    "                return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62636055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-25 16:56:17 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
      "I'm just a computer, so I don't have feelings, but I'm doing well! How about you?\n",
      "\n",
      "===== Inference Stats =====\n",
      "Time taken:       0.15 sec\n",
      "Token count:      16\n",
      "Throughput:       108.03 tokens/sec\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer, so I don't have feelings, but I'm doing well! How about you?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_stream('Hi there! How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c9c199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python script that reads filenames from a specified directory and saves them into a CSV file:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import csv\n",
      "from collections import namedtuple\n",
      "\n",
      "# Define the filename structure\n",
      "FileNamingSchema = namedtuple('FileNamingSchema', ['filename'])\n",
      "\n",
      "def get_filenames_from_dir(directory):\n",
      "    \"\"\"Get filenames from a directory.\"\"\"\n",
      "    return [file.split('_')[-1] for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\n",
      "\n",
      "def save_to_csv(filenames, output_csv):\n",
      "    \"\"\"Save filenames to a CSV file.\"\"\"\n",
      "    writer = csv.DictWriter(output_csv, fieldnames=FileNamingSchema._fields)\n",
      "    writer.writeheader()\n",
      "    for filename in filenames:\n",
      "        writer.writerow({'filename': filename})\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    directory = input(\"Enter the directory path: \")\n",
      "    output_csv = input(\"Enter the output CSV file name: \")\n",
      "    \n",
      "    try:\n",
      "        filenames = get_filenames_from_dir(directory)\n",
      "        save_to_csv(filenames, output_csv)\n",
      "        print(f\"Filenames successfully saved to {output_csv}\")\n",
      "    except FileNotFoundError:\n",
      "        print(f\"The directory '{directory}' does not exist.\")\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {str(e)}\")\n",
      "```\n",
      "\n",
      "**UsageINFO 11-25 16:56:27 [loggers.py:236] Engine 000: Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 34.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%\n",
      "**:\n",
      "\n",
      "1. Save the script to a file, e.g., `save_filenames.py`.\n",
      "2. Run the script using Python: `python save_filenames.py`\n",
      "3. Enter the directory path when prompted.\n",
      "4. Enter the name of the CSV file you want to create.\n",
      "\n",
      "This script uses the `csv` module to write the filenames to a CSV file. It assumes filenames follow a pattern where a hyphen (`_`) separates the filename and extension. If your files have different naming conventions, you may need to adjust the pattern in `get_filenames_from_dir`.\n",
      "\n",
      "**Note**: Ensure you have write permissions for the specified directory and directory name. The script also includes basic error handling for file not found and general exceptions.\n",
      "\n",
      "===== Inference Stats =====\n",
      "Time taken:       2.57 sec\n",
      "Token count:      229\n",
      "Throughput:       89.25 tokens/sec\n",
      "============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here\\'s a Python script that reads filenames from a specified directory and saves them into a CSV file:\\n\\n```python\\nimport os\\nimport csv\\nfrom collections import namedtuple\\n\\n# Define the filename structure\\nFileNamingSchema = namedtuple(\\'FileNamingSchema\\', [\\'filename\\'])\\n\\ndef get_filenames_from_dir(directory):\\n    \"\"\"Get filenames from a directory.\"\"\"\\n    return [file.split(\\'_\\')[-1] for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]\\n\\ndef save_to_csv(filenames, output_csv):\\n    \"\"\"Save filenames to a CSV file.\"\"\"\\n    writer = csv.DictWriter(output_csv, fieldnames=FileNamingSchema._fields)\\n    writer.writeheader()\\n    for filename in filenames:\\n        writer.writerow({\\'filename\\': filename})\\n\\nif __name__ == \\'__main__\\':\\n    directory = input(\"Enter the directory path: \")\\n    output_csv = input(\"Enter the output CSV file name: \")\\n    \\n    try:\\n        filenames = get_filenames_from_dir(directory)\\n        save_to_csv(filenames, output_csv)\\n        print(f\"Filenames successfully saved to {output_csv}\")\\n    except FileNotFoundError:\\n        print(f\"The directory \\'{directory}\\' does not exist.\")\\n    except Exception as e:\\n        print(f\"An error occurred: {str(e)}\")\\n```\\n\\n**Usage**:\\n\\n1. Save the script to a file, e.g., `save_filenames.py`.\\n2. Run the script using Python: `python save_filenames.py`\\n3. Enter the directory path when prompted.\\n4. Enter the name of the CSV file you want to create.\\n\\nThis script uses the `csv` module to write the filenames to a CSV file. It assumes filenames follow a pattern where a hyphen (`_`) separates the filename and extension. If your files have different naming conventions, you may need to adjust the pattern in `get_filenames_from_dir`.\\n\\n**Note**: Ensure you have write permissions for the specified directory and directory name. The script also includes basic error handling for file not found and general exceptions.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_stream('Write python code to read files names from a directory and save it as csv file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f3319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
