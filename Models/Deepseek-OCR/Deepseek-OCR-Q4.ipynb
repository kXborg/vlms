{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82cf83f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing flash_attn_2_cuda: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attn_func\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer, BitsAndBytesConfig\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\flashAttn\\lib\\site-packages\\flash_attn\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.7.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     flash_attn_func,\n\u001b[0;32m      5\u001b[0m     flash_attn_kvpacked_func,\n\u001b[0;32m      6\u001b[0m     flash_attn_qkvpacked_func,\n\u001b[0;32m      7\u001b[0m     flash_attn_varlen_func,\n\u001b[0;32m      8\u001b[0m     flash_attn_varlen_kvpacked_func,\n\u001b[0;32m      9\u001b[0m     flash_attn_varlen_qkvpacked_func,\n\u001b[0;32m     10\u001b[0m     flash_attn_with_kvcache,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\flashAttn\\lib\\site-packages\\flash_attn\\flash_attn_interface.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_triton_amd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interface_fa \u001b[38;5;28;01mas\u001b[39;00m flash_attn_gpu\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn_2_cuda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn_gpu\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_contiguous\u001b[39m(x):\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing flash_attn_2_cuda: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_func\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41828a83-1b0f-4a15-98ba-33edc891760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn version: 2.7.4\n",
      "Output shape: torch.Size([1, 8, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"flash-attn version:\", __import__('flash_attn').__version__)\n",
    "\n",
    "q = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Output shape:\", out.shape)  # Should be [1, 8, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce55c74a-e270-4f96-8253-ed44b5d4649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub transformers==4.46.3 tokenizers==0.20.3 einops addict easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508d0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbaba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Jalea96/DeepSeek-OCR-bnb-4bit-NF4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    ),\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    ").eval()\n",
    "\n",
    "print(\"4-bit model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d04441",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = '../../tasks/github-readme.png'\n",
    "output_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c827c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\flashAttn\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 100, 1280])\n",
      "=====================\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 21, 260, 66]]<|/det|>\n",
      "## TinyBot  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[44, 95, 960, 238]]<|/det|>\n",
      "Built to decide smart home actions intelligently, without explicit hard coding. TinyBot is a minimal, text- based conversational model designed to mimic basic understanding of home environment and your needs. It is similar to a chatbot but with an ultra- light neural core. Currently under development.  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 305, 222, 334]]<|/det|>\n",
      "## Features  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[68, 364, 848, 558]]<|/det|>\n",
      "- Understands simple conversational commands- Controls virtual smart-home devices- Friendly, human-like responses- Built using TensorFlow- Fits within microcontroller memory constraints (few MBs)- Action generator to be added soon  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 623, 562, 658]]<|/det|>\n",
      "## Supported Smart Devices  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[46, 686, 910, 740]]<|/det|>\n",
      "The bot can recognize and respond to queries about the following devices. More to be added soon.  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[68, 763, 344, 990]]<|/det|>\n",
      "- Fan- Light & Dim Light- Curtain- Door Sensor- Refrigerator- Phone Charger- Aquarium Filter\n",
      "==================================================\n",
      "image size:  (548, 842)\n",
      "valid image tokens:  766\n",
      "output texts tokens (valid):  286\n",
      "compression ratio:  0.37\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 7/7 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944708a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashAttn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
