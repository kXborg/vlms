{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cf83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_func\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41828a83-1b0f-4a15-98ba-33edc891760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn version: 2.7.4\n",
      "Output shape: torch.Size([1, 8, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"flash-attn version:\", __import__('flash_attn').__version__)\n",
    "\n",
    "q = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Output shape:\", out.shape)  # Should be [1, 8, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55c74a-e270-4f96-8253-ed44b5d4649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub transformers==4.46.3 tokenizers==0.20.3 einops addict easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508d0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8500e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Downloading shards: 100%|██████████| 1/1 [00:15<00:00, 15.37s/it]\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d04441",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = '../../tasks/github-readme.png'\n",
    "output_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c827c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\flashAttn\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 100, 1280])\n",
      "=====================\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 21, 260, 66]]<|/det|>\n",
      "## TinyBot  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[44, 95, 960, 238]]<|/det|>\n",
      "Built to decide smart home actions intelligently, without explicit hard coding. TinyBot is a minimal, text- based conversational model designed to mimic basic understanding of home environment and your needs. It is similar to a chatbot but with an ultra- light neural core. Currently under development.  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 305, 222, 334]]<|/det|>\n",
      "## Features  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[68, 364, 848, 558]]<|/det|>\n",
      "- Understands simple conversational commands- Controls virtual smart-home devices- Friendly, human-like responses- Built using TensorFlow- Fits within microcontroller memory constraints (few MBs)- Action generator to be added soon  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 623, 562, 658]]<|/det|>\n",
      "## Supported Smart Devices  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[46, 686, 910, 740]]<|/det|>\n",
      "The bot can recognize and respond to queries about the following devices. More to be added soon.  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[68, 763, 344, 990]]<|/det|>\n",
      "- Fan- Light & Dim Light- Curtain- Door Sensor- Refrigerator- Phone Charger- Aquarium Filter\n",
      "==================================================\n",
      "image size:  (548, 842)\n",
      "valid image tokens:  766\n",
      "output texts tokens (valid):  286\n",
      "compression ratio:  0.37\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 7/7 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944708a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashAttn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
