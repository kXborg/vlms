{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8759c4e-8043-44ce-adb3-6e12f4ee21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e29074a-5a30-4c0a-8a11-e67a5e1c61be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 14:31:02 [utils.py:253] non-default args: {'enable_prefix_caching': False, 'disable_log_stats': True, 'mm_processor_cache_gb': 0, 'logits_processors': [<class 'vllm.model_executor.models.deepseek_ocr.NGramPerReqLogitsProcessor'>], 'model': 'deepseek-ai/DeepSeek-OCR'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dcaa4e8b754bac95e6dc35f4b8bf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 14:31:11 [model.py:630] Resolved architecture: DeepseekOCRForCausalLM\n",
      "INFO 11-12 14:31:11 [model.py:1728] Using max model len 8192\n",
      "INFO 11-12 14:31:14 [scheduler.py:254] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfede5c209b42a985075655bfa644a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb79f838210482498b9a947858b870e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeff0c712c84f459f372467831e956f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:16 [core.py:94] Initializing a V1 LLM engine (v0.11.1rc7.dev73+ga9d18b510) with config: model='deepseek-ai/DeepSeek-OCR', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-OCR', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=deepseek-ai/DeepSeek-OCR, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': True, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:18 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fa5436280e4d88bc5dbfd80f96b131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:22 [gpu_model_runner.py:3047] Starting to load model deepseek-ai/DeepSeek-OCR...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:23 [layer.py:573] MultiHeadAttention attn_backend: AttentionBackendEnum.FLASH_ATTN, use_upstream_fa: False\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m WARNING 11-12 14:31:23 [rocm.py:34] Failed to import from amdsmi with ModuleNotFoundError(\"No module named 'amdsmi'\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m WARNING 11-12 14:31:23 [rocm.py:45] Failed to import from vllm._rocm_C with ModuleNotFoundError(\"No module named 'vllm._rocm_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/.venv/lib/python3.12/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py\", line 836, in <module>\n",
      "    main()\n",
      "  File \"/.venv/lib/python3.12/site-packages/tvm_ffi/utils/_build_optional_torch_c_dlpack.py\", line 829, in main\n",
      "    build_ninja(build_dir=str(build_dir))\n",
      "  File \"/.venv/lib/python3.12/site-packages/tvm_ffi/cpp/extension.py\", line 344, in build_ninja\n",
      "    status = subprocess.run(check=False, args=command, cwd=build_dir, capture_output=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 1955, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ninja'\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m /.venv/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:106: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m You may try AOT-module via `pip install torch-c-dlpack-ext`\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:25 [cuda.py:415] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:25 [cuda.py:424] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746e8ea6a6a24503a2bf6fa5a5f080c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:31:58 [weight_utils.py:440] Time spent downloading weights for deepseek-ai/DeepSeek-OCR: 32.939428 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da99f8e7fa4465a8ae2e88d50dc318c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ec2a86858a43a3b63594acf2cef0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:00 [default_loader.py:314] Loading weights took 2.01 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:01 [gpu_model_runner.py:3126] Model loading took 6.2319 GiB memory and 37.827877 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:01 [gpu_model_runner.py:3876] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 11 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:07 [backends.py:618] Using cache directory: /root/.cache/vllm/torch_compile_cache/3856c240a2/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:07 [backends.py:634] Dynamo bytecode transform time: 3.53 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:09 [backends.py:250] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:21 [backends.py:281] Compiling a graph for dynamic shape takes 14.08 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m /.venv/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS=\"+dynamo\" for a DEBUG stack trace.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m   torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m WARNING 11-12 14:32:24 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=896,device_name=NVIDIA_A40.json']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:25 [monitor.py:34] torch.compile takes 17.61 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:26 [gpu_worker.py:353] Available KV cache memory: 31.96 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:27 [kv_cache_utils.py:1229] GPU KV cache size: 558,560 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:27 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 68.18x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:12<00:00,  4.16it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:41 [gpu_model_runner.py:4032] Graph capturing finished in 15 secs, took 0.76 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3262)\u001b[0;0m INFO 11-12 14:32:41 [core.py:247] init engine (profile, create kv cache, warmup model) took 40.10 seconds\n",
      "INFO 11-12 14:32:43 [llm.py:353] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "llm = LLM(\n",
    "    model=\"deepseek-ai/DeepSeek-OCR\",\n",
    "    enable_prefix_caching=False,\n",
    "    mm_processor_cache_gb=0,\n",
    "    logits_processors=[NGramPerReqLogitsProcessor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b429f131-f0e0-425c-9148-1f2e62c3ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vlms'...\n",
      "remote: Enumerating objects: 326, done.\u001b[K\n",
      "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
      "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
      "remote: Total 326 (delta 42), reused 101 (delta 14), pack-reused 191 (from 1)\u001b[K\n",
      "Receiving objects: 100% (326/326), 63.55 MiB | 13.81 MiB/s, done.\n",
      "Resolving deltas: 100% (111/111), done.\n",
      "Updating files: 100% (124/124), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kXborg/vlms.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9616692f-9ec6-4a2f-83b7-96325c60a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\tUntitled1.ipynb  vlms\n",
      "/workspace/vlms\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "%cd vlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2289dcce-cde9-49ff-960e-0916a59f5cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.jpg\t11.jpg\t13.jpg\t3.jpg  5.jpg  7.jpg  9.jpg\n",
      "10.jpg\t12.jpg\t2.jpg\t4.jpg  6.jpg  8.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls tasks/pdf-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07dc0cb2-d655-4488-8eff-a8e593858b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334dcec6cf2147468e56464564c2f04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8ef86025d54cd4b265218a16b26c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaliGemma 2 Mix: For Downstream Tasks\n",
      "\n",
      "PaliGemma 2 is an advanced vision-language model (VLM) developed by Google, designed to seamlessly integrate visual and textual data processing. Building upon the foundations of the original PaliGemma, this iteration combines the **SigLIP** vision encoder with the **Gemma 2** language model, resulting in a powerful tool capable of handling a diverse array of multimodal tasks.\n",
      "\n",
      "What is PaliGemma 2 Mix?\n",
      "\n",
      "PaliGemma 2 Mix refers to a set of models fine-tuned on a mixture of vision-language tasks, enabling robust zero-shot performance across various applications without the need for extensive task-specific training. This versatility makes it suitable for tasks such as:\n",
      "\n",
      "- **Image Captioning**: Generating both short and long-form descriptions of images.\n",
      "- **Optical Character Recognition (OCR)**: Extracting and interpreting text from images.\n",
      "- **Visual Question Answering (VQA)**: Answering questions based on image content.\n",
      "- **Object Detection and Segmentation**: Identifying and delineating objects within images.\n",
      "- **Document Understanding**: Interpreting and summarizing information from documents.\n",
      "- **Text Recognition within Images**: Detecting and reading embedded text in visuals.\n",
      "\n",
      "Model Architecture\n",
      "\n",
      "The architecture of PaliGemma 2 Mix is a fusion of:\n",
      "\n",
      "- **SigLIP Vision Encoder**: Processes images into visual tokens.\n",
      "- **Linear Projection Layer**: Aligns visual tokens with textual inputs.\n",
      "- **Gemma 2 Language Model**: Generates coherent textual outputs based on combined inputs.\n",
      "This design allows the model to process both images and text inputs, facilitating a wide range of vision-language tasks.\n",
      "\n",
      "**Model Variants and Input Resolutions**\n",
      "\n",
      "PaliGemma 2 Mix is available in multiple configurations to cater to different computational resources and application needs:\n",
      "\n",
      "| Model Size | Supported Input Resolutions |\n",
      "|------------|------------------------------|\n",
      "| 3B         | 224x224, 448x448             |\n",
      "| 10B        | 224x224, 448x448             |\n",
      "| 28B        | 224x224, 448x448             |\n",
      "\n",
      "Note: While the 10B and 28B models offer enhanced performance, they require more substantial computational resources. For instance, running the 10B model in full precision may not be feasible on platforms like Colab T4. However, utilizing bfloat16 precision can allow for more efficient loading and execution.\n",
      "\n",
      "From our testing, the PaliGemma 2 Mix 3B model at 448x448 resolution offers an excellent balance between performance and computational efficiency. It falls under category of Small VLMs. For specialized tasks or improved performance, further fine-tuning on domain-specific data can enhance the model's capabilities.\n",
      "\n",
      "Read more: Link\n",
      "\n",
      "In [ ]: from huggingface_hub import notebook_login\n",
      "\n",
      "notebook_login()\n",
      "\n",
      "# This is a gated model. Visit the PaliGemma2Mix model card and accept the Licen\n",
      "# https://huggingface.co/google/paliGemma2-3b-mix-448\n",
      "# Then, pass your Hugging Face token.\n"
     ]
    }
   ],
   "source": [
    "# Prepare batched input with your image file\n",
    "image_1 = Image.open(\"./tasks/pdf-images/1.jpg\").convert(\"RGB\")\n",
    "image_2 = Image.open(\"./tasks/pdf-images/2.jpg\").convert(\"RGB\")\n",
    "prompt = \"<image>\\nFree OCR.\"\n",
    "\n",
    "model_input = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_1}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_2}\n",
    "    }\n",
    "]\n",
    "\n",
    "sampling_param = SamplingParams(\n",
    "            temperature=0.0,\n",
    "            max_tokens=8192,\n",
    "            # ngram logit processor args\n",
    "            extra_args=dict(\n",
    "                ngram_size=30,\n",
    "                window_size=90,\n",
    "                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n",
    "            ),\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "# Generate output\n",
    "model_outputs = llm.generate(model_input, sampling_param)\n",
    "\n",
    "# Print output\n",
    "for output in model_outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f00f8-da29-4ba2-8aa6-319869bc3d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
